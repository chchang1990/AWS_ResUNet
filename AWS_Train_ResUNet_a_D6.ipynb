{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "xyJpCsOXVFC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, os, sys\n",
        "import sagemaker, boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sagemaker.tensorflow import TensorFlow\n",
        "\n",
        "\n",
        "sess = boto3.Session()\n",
        "sm   = sess.client('sagemaker')\n",
        "role = sagemaker.get_execution_role()\n",
        "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
        "\n",
        "# # ----- Upload TFRecords to S3 bucket first -----\n",
        "# # You can upload the data to S3 in the SageMaker\n",
        "# datasets_bucketpath = sagemaker_session.upload_data(path='cifar10', key_prefix='datasets/cifar10-dataset')\n",
        "# # Or directly upload the data from your local computer to S3 without going through SageMaker. In this case, provide the bucket path\n",
        "datasets_bucketpath = 's3://ai4boundaries'\n",
        "\n",
        "\n",
        "\n",
        "from smexperiments.experiment import Experiment\n",
        "from smexperiments.trial import Trial\n",
        "from smexperiments.trial_component import TrialComponent\n",
        "\n",
        "training_experiment = Experiment.create(\n",
        "                                experiment_name = \"sagemaker-training-experiments\",\n",
        "                                description     = \"Experiment to track ResUnet training trials\",\n",
        "                                sagemaker_boto_client=sm)\n",
        "\n",
        "single_gpu_trial = Trial.create(\n",
        "    trial_name = 'sagemaker-single-gpu-training',\n",
        "    experiment_name = training_experiment.experiment_name,\n",
        "    sagemaker_boto_client = sm,\n",
        ")\n",
        "\n",
        "trial_comp_name = 'single-gpu-training-job'\n",
        "experiment_config = {\"ExperimentName\": training_experiment.experiment_name,\n",
        "                       \"TrialName\": single_gpu_trial.trial_name,\n",
        "                       \"TrialComponentDisplayName\": trial_comp_name}\n",
        "\n",
        "\n",
        "\n",
        "# # ----- Train the model with fixed hyperparameters -----\n",
        "hyperparams={'epochs'       : 5,\n",
        "             'learning-rate': 0.01,\n",
        "             'batch-size'   : 8}\n",
        "\n",
        "bucket_name = sagemaker_session.default_bucket()\n",
        "output_path = f's3://{bucket_name}/jobs'\n",
        "\n",
        "# Check how to revise this\n",
        "metric_definitions = [{'Name': 'val_extent_accuracy', 'Regex': 'val_extent_accuracy: ([0-9\\\\.]+)'}]\n",
        "\n",
        "tf_estimator = TensorFlow(entry_point          = 'resunet-training-sagemaker.py',\n",
        "                          output_path          = f'{output_path}/',\n",
        "                          code_location        = output_path,\n",
        "                          role                 = role,\n",
        "                          train_instance_count = 1,\n",
        "                          train_instance_type  = 'ml.g4dn.xlarge',\n",
        "                          framework_version    = '1.15.2',\n",
        "                          py_version           = 'py3',\n",
        "                          input_mode           = 'Pipe',\n",
        "#                          script_mode          = True,\n",
        "                          metric_definitions   = metric_definitions,\n",
        "                          sagemaker_session    = sagemaker_session,\n",
        "                          hyperparameters      = hyperparams)\n",
        "\n",
        "job_name=f'tensorflow-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
        "tf_estimator.fit({'training'  : datasets_bucketpath,\n",
        "                  'validation': datasets_bucketpath,\n",
        "                  'eval'      : datasets_bucketpath},\n",
        "                 job_name = job_name,\n",
        "                 experiment_config=experiment_config)\n"
      ],
      "metadata": {
        "id": "Uj1EPFVxd530"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOlkyinfd6Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model tuning"
      ],
      "metadata": {
        "id": "v6pcAjKAdpJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, os, sys\n",
        "import sagemaker, boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sagemaker.tensorflow import TensorFlow\n",
        "\n",
        "\n",
        "sess = boto3.Session()\n",
        "sm   = sess.client('sagemaker')\n",
        "role = sagemaker.get_execution_role()\n",
        "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
        "\n",
        "# # ----- Upload TFRecords to S3 bucket first -----\n",
        "# # You can upload the data to S3 in the SageMaker\n",
        "# datasets_bucketpath = sagemaker_session.upload_data(path='cifar10', key_prefix='datasets/cifar10-dataset')\n",
        "# # Or directly upload the data from your local computer to S3 without going through SageMaker. In this case, provide the bucket path\n",
        "datasets_bucketpath = 's3://.......'\n",
        "\n",
        "\n",
        "\n",
        "# # ----- Automatic model-tuning -----\n",
        "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
        "\n",
        "\n",
        "objective_metric_name = ['val_extent_accuracy']\n",
        "objective_type = ['Maximize']\n",
        "\n",
        "# Check how to revise this\n",
        "metric_definitions = [{'Name': 'val_extent_accuracy', 'Regex': 'val_extent_accuracy: ([0-9\\\\.]+)'}]\n",
        "\n",
        "\n",
        "tf_estimator = TensorFlow(entry_point          = 'resunet-training-sagemaker.py',\n",
        "                          output_path          = f'{output_path}/',\n",
        "                          code_location        = output_path,\n",
        "                          role                 = role,\n",
        "                          train_instance_count = 1,\n",
        "                          train_instance_type  = 'ml.g4dn.xlarge',\n",
        "                          framework_version    = '1.15',\n",
        "                          py_version           = 'py3',\n",
        "                          script_mode          = True,\n",
        "                          metric_definitions   = metric_definitions,\n",
        "                          sagemaker_session    = sagemaker_session)\n",
        "\n",
        "\n",
        "hyperparameter_ranges = {\n",
        "    'epochs'        : IntegerParameter(250, 300),\n",
        "    'learning-rate' : ContinuousParameter(0.001, 0.1, scaling_type='Logarithmic'),\n",
        "    'batch-size'    : CategoricalParameter(['8','16','32']),\n",
        "}\n",
        "\n",
        "tuner = HyperparameterTuner(estimator             = tf_estimator,\n",
        "                            objective_metric_name = objective_metric_name,\n",
        "                            hyperparameter_ranges = hyperparameter_ranges,\n",
        "                            metric_definitions    = metric_definitions,\n",
        "                            max_jobs              = 16,\n",
        "                            max_parallel_jobs     = 8,\n",
        "                            objective_type        = objective_type)\n",
        "\n",
        "job_name=f'tf-hpo-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
        "tuner.fit({'training'  : datasets_bucketpath,\n",
        "           'validation': datasets_bucketpath,\n",
        "           'eval'      : datasets_bucketpath},\n",
        "            job_name = job_name)"
      ],
      "metadata": {
        "id": "S0I0Hva-a0kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VdXw0nSKf_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
